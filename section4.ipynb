{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"section4.ipynb","provenance":[],"toc_visible":true,"mount_file_id":"1CtPZyW1FdeZiEpH48AcCbWHrrNTQ1zzl","authorship_tag":"ABX9TyPATXjFgBmRVc+B7IFZJNlH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ニューラルネットワークの学習"],"metadata":{"id":"6s5w2EiU_aVl"}},{"cell_type":"markdown","source":["## 4.1 データから学習する<83>\n"],"metadata":{"id":"JpFGeLMb_ak5"}},{"cell_type":"markdown","source":["「学習」とは、訓練データから最適な重みパラメータの値を自動取得すること。\n","\n","そのための指標として**損失関数**を導入してその値が小さくなるように重みパラメータを探し出すということが学習の目的。"],"metadata":{"id":"T3HCqV9L_aoF"}},{"cell_type":"markdown","source":["### 4.1.1 データ駆動\n","\n"],"metadata":{"id":"ykDfip7d_aqq"}},{"cell_type":"markdown","source":["データ駆動とは、データを元に次のアクションを決めたり、意思決定を行ったりすることです。\n","\n","機械学習による手法では、人の介入を避け、集められたデータからパターンを求めようとする。"],"metadata":{"id":"kcFbp2e2_atX"}},{"cell_type":"markdown","source":["たとえば、「５」を認識するアルゴリズムを\n","\n","**特徴量**を抽出して、その特徴量のパターンを機械学習の技術で学習する方法が考えられる。（特徴量とは、入力データから本質的なデータを的確に抽出できるように設計された変換機）"],"metadata":{"id":"X7DQDia9_av4"}},{"cell_type":"markdown","source":["### 4.1.2 訓練データとテストデータ<86>\n"],"metadata":{"id":"nKH5TMyv1cAJ"}},{"cell_type":"markdown","source":["機械学習の流れ\n","\n","**訓練データ**だけを使って学習を行い最適なパラメータを探索\n","\n","→**テストデータ**を使って訓練したモデルの実力を評価\n","\n","なぜこの流れなのか？\n","\n","それは**汎化性能**を高めるため\n","\n","汎化性能...まだ見ぬデータにタする能力"],"metadata":{"id":"T1nUAQT62UmZ"}},{"cell_type":"markdown","source":["## 4.2 損失関数 <87>"],"metadata":{"id":"rAeCFIX51b9J"}},{"cell_type":"markdown","source":["NNが最適なパラメータを探すうえで指標になるものが**損失関数**"],"metadata":{"id":"950IrRkj1b5o"}},{"cell_type":"markdown","source":["### 4.2.1 2乗和誤差(sum of squared error)<88>"],"metadata":{"id":"W8_SpVPk2wdk"}},{"cell_type":"markdown","source":["2乗和誤差は式(4.1)で書けるようにNNの出力と、正解となる教師データの各要素の差の2乗を計算して総和を求める。\n","\n","$$E = \\frac{1}{2}\\sum_{k}(y_k - t_k)^2 \\hspace{10mm} (4.1)$$\n","\n","実装は以下である"],"metadata":{"id":"oUDdSnoU2wa9"}},{"cell_type":"code","source":["import numpy as np\n","def sum_squared_error(y, t):\n","  return 0.5 * np.sum((y - t) ** 2)\n","\n","#「2」を正解としたone-hot表現\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","\n","#「2」の確率が一番高い(0.6)例\n","y1 =  [0.1, 0.05, 0.6, 0, 0.05, 0.1, 0, 0.1, 0, 0]\n","\n","#「7」の確率が一番高い(0.6)例\n","y2 =  [0.1, 0.05, 0.1, 0, 0.05, 0.1, 0, 0.6, 0, 0]\n","\n","print(f\"「2」の確率が一番高い(0.6)例のMAE: {sum_squared_error(np.array(y1), np.array(t))}\")\n","print(f\"「7」の確率が一番高い(0.6)例のMAE: {sum_squared_error(np.array(y2), np.array(t))}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbZM5RTz4HLN","executionInfo":{"status":"ok","timestamp":1639637802692,"user_tz":-540,"elapsed":12,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"outputId":"2ca8e4fa-9bd6-4805-a49b-eef3bc9be989"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["「2」の確率が一番高い(0.6)例のMAE: 0.09750000000000003\n","「7」の確率が一番高い(0.6)例のMAE: 0.5975\n"]}]},{"cell_type":"markdown","source":["実行結果の上のほうがMAEの値が低いので誤差が少ないことが容易にわかる。"],"metadata":{"id":"qzw_vN9B2wYh"}},{"cell_type":"markdown","source":["### 4.2.2 交差エントロピー誤差"],"metadata":{"id":"JKbgWbNZ2wVD"}},{"cell_type":"markdown","source":["交差エントロピー誤差は以下の式で書ける。\n","\n","$$E = -\\sum_{k}t_k \\mathrm{log}y_k \\hspace{10mm} (4.2)$$\n","\n","$t_k$は正解ラベルであるから、実質正解したものだけを対数和とってるだけだね。\n"],"metadata":{"id":"YHsaJEjD2wSO"}},{"cell_type":"markdown","source":["例題を使って交差エントロピー誤差をもう少し理解\n","\n","例：$(t_0, t_1, t_2) = (1, 0, 0)$のとき\n","\n","(1)出力値$(y_0, y_1, y_2) = (0.8, 0.1, 0.1)$\n","\n","$E = -1 * \\mathrm{log}0.8 - 0 *\\mathrm{log}0.1 - 0 *\\mathrm{log}0.1 = 0.09$  \n","\n","(2)出力値$(y_0, y_1, y_2) = (0.2, 0.7, 0.1)$\n","\n","$E = -1 * \\mathrm{log}0.2 - 0 *\\mathrm{log}0.7 - 0 *\\mathrm{log}0.1 = 1.61$\n","\n","しっかりと誤差大きければ値が大きくなっててうれしい。"],"metadata":{"id":"znmoo6QW2wPv"}},{"cell_type":"markdown","source":["では、ここからは実装に入っていく。"],"metadata":{"id":"dYf_mcWN8yp2"}},{"cell_type":"code","source":["def cross_entropy_error(y, t):\n","  delta = 1e-7 #y = 0をlogに入れるわけにはいかないのでちょびっと足しておく\n","  return -np.sum(t * np.log(y + delta))  \n","\n","#「2」を正解としたone-hot表現\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","\n","#「2」の確率が一番高い(0.6)例\n","y1 =  [0.1, 0.05, 0.6, 0, 0.05, 0.1, 0, 0.1, 0, 0]\n","\n","#「7」の確率が一番高い(0.6)例\n","y2 =  [0.1, 0.05, 0.1, 0, 0.05, 0.1, 0, 0.6, 0, 0]\n","\n","print(f\"「2」の確率が一番高い(0.6)例のMAE: {cross_entropy_error(np.array(y1), np.array(t))}\")\n","print(f\"「7」の確率が一番高い(0.6)例のMAE: {cross_entropy_error(np.array(y2), np.array(t))}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"scnC0Cpd8xsH","executionInfo":{"status":"ok","timestamp":1639637802692,"user_tz":-540,"elapsed":7,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"outputId":"7f47ea01-03bb-4a3c-ebc1-b76d0dbf9400"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["「2」の確率が一番高い(0.6)例のMAE: 0.510825457099338\n","「7」の確率が一番高い(0.6)例のMAE: 2.302584092994546\n"]}]},{"cell_type":"markdown","source":["### 4.2.3 ミニバッチ学習\n","\n","ここまでは一つのデータの損失関数の話で、こっからは訓練データ全体の損失関数が知りたいね。その際は訓練データ全部の損失関数の和が指標となるよね"],"metadata":{"id":"eLNCAG0P2wND"}},{"cell_type":"markdown","source":["上にある交差エントロピー誤差を訓練データすべての和の形に直すと以下の式になる\n","\n","$$E = -\\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk} \\mathrm{log}y_{nk} \\hspace{10mm} (4.3)$$\n","\n","$N$...データ数\n","\n","$t_{nk},y_{nk}$...n個目のデータのk番目の値\n","\n","$\\frac{1}{N}$でわっている理由は正規化ね。これをすることにより、一個当たりの損失関数という見方ができる\n"],"metadata":{"id":"XbaXyxhe2wKT"}},{"cell_type":"markdown","source":["ミニバッチってなに？\n","\n","→データ数が多すぎるとすべてを対象とした損失関数を求めるのは現実的じゃない。だから、データの一部を取り出して、そいつらで全体を近似する。この訓練だーたから選びだされた小さな塊を**ミニバッチ**とい。\n","\n","なんか、統計学でいうところの標本みたいなもんだな。"],"metadata":{"id":"DaZ0OvbR2vnF"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"a-9DXfnFCTqp"}},{"cell_type":"markdown","source":["### 4.2.4[バッチ対応版]交差エントロピー誤差実装<94>"],"metadata":{"id":"C1eEV0otCbs1"}},{"cell_type":"markdown","source":["4.2.3 ミニバッチ学習でやったことを用いて交差エントロピー誤差を実装していく。"],"metadata":{"id":"NsdSwYeACmfW"}},{"cell_type":"code","source":["#まずデータのload\n","import sys,os\n","sys.path.append(\"/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master\")\n","from dataset.mnist import load_mnist\n","\n","(x_train, t_train), (x_test, t_test) = \\\n","  load_mnist(flatten=True, normalize=False)\n","\n","#それぞれのデータの形状を出力\n","print(f\"x_train.shape: {x_train.shape}\")\n","print(f\"t_train.shape: {t_train.shape}\")\n","print(f\"x_test.shape: {x_test.shape}\")\n","print(f\"t_test.shape: {t_test.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MKLDmL5CtyH","executionInfo":{"status":"ok","timestamp":1639637803067,"user_tz":-540,"elapsed":379,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"outputId":"9bc1164a-a23a-4404-d549-e9f9416374ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x_train.shape: (60000, 784)\n","t_train.shape: (60000,)\n","x_test.shape: (10000, 784)\n","t_test.shape: (10000,)\n"]}]},{"cell_type":"code","source":["#ミニバッチを定義する\n","train_size = x_train.shape[0]\n","batch_size = 10\n","batch_mask = np.random.choice(train_size, batch_size)\n","x_batch = x_train[batch_mask]\n","t_batch = t_train[batch_mask]\n","print(f\"batch_mask: {batch_mask}\")"],"metadata":{"id":"ef47ZtrJC6H0","executionInfo":{"status":"ok","timestamp":1639637803067,"user_tz":-540,"elapsed":5,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"83c9cc85-c40f-4925-f4ff-cf1cdb33f6eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["batch_mask: [48072 10978  8025 30051  6285  3868 35832 58489 36023 27493]\n"]}]},{"cell_type":"markdown","source":["??batch_maskってなんでこんな名前なの"],"metadata":{"id":"lOIJbV7FCmcd"}},{"cell_type":"code","source":["#バッチ対応版交差エントロピー誤差を定義\n","def cross_entropy_error(y, t):\n","  if y.ndim ==1:\n","    t = t.reshape(1, t.size)\n","    y = y.reshape(1, y.size)\n","\n","  batch_size = y.shape[0]\n","  return -np.sum(np.log(y[np.arrange(batch_size), t])) / batch_size"],"metadata":{"id":"d9LakBeyCgGV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["コード補足\n","\n","```\n","np.log(y[np.arrange(batch_size), t])\n","```\n","np.arrange(batch_size)で0~batch_size -1までの配列を生成\n","\n","例としてbatch_size = 5とすると\n","\n","np.arrange(batch_size)は、[0, 1, 2, 3, 4]"],"metadata":{"id":"lvEz--wuCmZk"}},{"cell_type":"markdown","source":["### 4.2.5 なぜ損失関数を設定するのか\n","損失関数なんかなくても認識精度だけ見ておけばいいじゃないかという疑問があるかもしれないね。\n","\n","では、なぜ損失関数を定義するのか、それは最適なパラメータを見つける時に損失関数が小さくなるように探しているからだよ。\n","\n","もっというと、最適なパラメータ更新はパラメータの勾配(微分)を計算して求めているから。\n","\n","逆に、なぜ認識精度を指標にしてはいけないかというと、**ほとんどの場所で微分が０**になるから！\n","→認識精度は離散的な値を取るので微分の意味がなくなる。"],"metadata":{"id":"7kNlFjNyCl54"}},{"cell_type":"markdown","source":["## 数値微分<97>"],"metadata":{"id":"yxNsNZ6fvl4X"}},{"cell_type":"markdown","source":["### 4.3.1 微分<97>"],"metadata":{"id":"kvxtiLt0vly8"}},{"cell_type":"markdown","source":["微分の定義通りに関数を実装してみる"],"metadata":{"id":"brT1dBB2vlxG"}},{"cell_type":"code","source":["#ダメな微分の定義\n","def numerical_diff(f, x):\n","  h = 10e-50\n","  return (f(x + h) - f(x)) / h"],"metadata":{"id":"0KYZXLXgwQEO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["定義通りに実装したら上のようになるが何がダメなんだろうか？\n","\n","上関数のダメポイントは２つある。\n","\n","1.分母が小さすぎる。\n","\n","→大体$10^{-4}$くらいがうまく行くらしい。\n","\n","2.分子に誤差が生まれる\n","\n","→hを無限に小さくすることができないので、誤差が生まれてくる。\n","\n","対策として(x + h)と(x - h)をとるとx中心の誤差となり、誤差が減少される。\n","\n","これらを考慮して微分の式を定義し直すと"],"metadata":{"id":"g_mVDb3Xvlvg"}},{"cell_type":"code","source":["def numerical_diff(f, x):\n","  h = 1e-4\n","  return (f(x + h) - f(x - h)) / (2*h)"],"metadata":{"id":"mYJF2NqezgzP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.3.2 数値微分の例<100>\n"],"metadata":{"id":"jmXc0MVfvltM"}},{"cell_type":"markdown","source":["次の２次関数のx = 5,10における接線を求めてみよう\n","\n","$$ y = 0.01x^{2} + 0.1x \\hspace{10mm} (4.5)$$"],"metadata":{"id":"rNA2eVaZvlrE"}},{"cell_type":"code","source":["##式(4.5)を定義\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def function1(x):\n","  return 0.01*x**2 + 0.1*x\n","\n","diff1 = numerical_diff(function1, 5)\n","diff2 = numerical_diff(function1, 10)\n","\n","x = np.arange(0, 20, 0.1)\n","y = function1(x)\n","y1 = diff1 * (x - 5) + function1(5)\n","y2 = diff2 * (x - 10) + function1(10)\n","\n","plt.xlabel(\"x\")\n","plt.ylabel(\"f(x)\")\n","\n","plt.plot(x, y)\n","plt.plot(x,y1)\n","plt.plot(x,y2)\n","\n","plt.show()"],"metadata":{"id":"DpFnEmlt0grz","executionInfo":{"status":"ok","timestamp":1639637803434,"user_tz":-540,"elapsed":370,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fadaae8-c392-4bd8-9436-5f32c275bf3c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### 4.3.3 偏微分<102>"],"metadata":{"id":"RYgKwhSzvlow"}},{"cell_type":"markdown","source":["$$ f(x_0, x_1) = x_0^{2} + x_1^{2} \\hspace{10mm} (4.6)$$\n","を偏微分することを考えよう\n","\n","問1:$x_0= 3,x_1 = 4$の時$x_0$に対する偏微分$\\frac{\\partial f}{\\partial x_0}$\n","\n","問2:$x_0= 3,x_1 = 4$の時$x_1$に対する偏微分$\\frac{\\partial f}{\\partial x_1}$"],"metadata":{"id":"HoVkMTBUvljj"}},{"cell_type":"code","source":["#問1\n","def function_tmp1(x0):\n","  return x0*x0 + 4.0**2.0\n","\n","print(f\"問1の回答: {numerical_diff(function_tmp1, 3.0)}\")\n","\n","#問2\n","def function_tmp2(x1):\n","  return 3.0**2.0 + x1*x1\n","\n","print(f\"問2の回答: {numerical_diff(function_tmp2, 4.0)}\")"],"metadata":{"id":"s2z1JGot4iuW","executionInfo":{"status":"ok","timestamp":1639637803434,"user_tz":-540,"elapsed":11,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2376544a-1df5-45dd-f70a-dc104c474383"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["問1の回答: 6.00000000000378\n","問2の回答: 7.999999999999119\n"]}]},{"cell_type":"markdown","source":["## 4.4 勾配<103>"],"metadata":{"id":"Au9fO1PYvlhc"}},{"cell_type":"markdown","source":["上の偏微分では特定の値に対して偏微分を求めた。\n","\n","ここから一般の変数に対して偏微分を求めることを考えよう。"],"metadata":{"id":"9_rGRCR-vlfH"}},{"cell_type":"markdown","source":["$(\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1} )$のように全ての変数の偏微分をベクトルとしてまとめたものを**勾配(gradient)**という。\n","\n","勾配の実装は以下のようにできる。"],"metadata":{"id":"QLEcJ5fZvldJ"}},{"cell_type":"code","source":["#勾配の実装\n","#まず勾配を求める関数の定義\n","def function_2(x):\n","  return x[0]**2 + x[1]**2\n","\n","def numerical_gradient(f, x):\n","  h = 1e-4\n","  grad = np.zeros_like(x) #xと同じ形状の配列でその要素が全て0\n","\n","  for idx in range(x.size):\n","    tmp_val = x[idx]\n","    #f(x + h)の計算\n","    x[idx] = tmp_val + h\n","    fxh1 = f(x)\n","\n","    #f(x - h)の計算\n","    x[idx] = tmp_val - h\n","    fxh2 = f(x)\n","\n","    grad[idx] = (fxh1 - fxh2) / (2*h)\n","    x[idx] = tmp_val\n","\n","  return grad\n","\n","print(f\"function_2の(3,4)での勾配: {numerical_gradient(function_2, np.array([3.0, 4.0]))} \")\n","print(f\"function_2の(0,2)での勾配: {numerical_gradient(function_2, np.array([0.0, 2.0]))} \")\n","print(f\"function_2の(3,0)での勾配: {numerical_gradient(function_2, np.array([3.0, 0.0]))} \")"],"metadata":{"id":"wxInaYelvlDE","executionInfo":{"status":"ok","timestamp":1639637803434,"user_tz":-540,"elapsed":10,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"243eca1e-65e8-4e99-e9b4-951ce7a898d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["function_2の(3,4)での勾配: [6. 8.] \n","function_2の(0,2)での勾配: [0. 4.] \n","function_2の(3,0)での勾配: [6. 0.] \n"]}]},{"cell_type":"markdown","source":["勾配が示す方向は、各場所において**関数の値を最も減らす方向**"],"metadata":{"id":"pf8LGU_JCQWn"}},{"cell_type":"markdown","source":["### 4.1.1 勾配法<106>"],"metadata":{"id":"0p9OLMwmCbm4"}},{"cell_type":"markdown","source":["**勾配法**とは、勾配を利用して関数の最小値を探そうとする方法。\n","\n","ただし、勾配が示す方向が必ず最小値なのかどうかはわからないので注意が必要。"],"metadata":{"id":"aIDh68asCsIR"}},{"cell_type":"markdown","source":["勾配法の式は以下で書ける。\n","$$x_0 = x_0 - ɳ\\frac{\\partial f}{\\partial x_0} \\\\ x_1 = x_1 - ɳ\\frac{\\partial f}{\\partial x_1}  \\\\ (4.7)$$\n","\n","$ɳ$は更新の量を表しており、NNの学習においては**学習率**と呼ばれる。\n","\n","これは、１回の学習でどれだけパラメータを更新するかを決める量。"],"metadata":{"id":"10bTmEumDPAE"}},{"cell_type":"markdown","source":["勾配降下法を実装してみよう。"],"metadata":{"id":"-NAmTDUuDO9-"}},{"cell_type":"code","source":["def gradient_descent(f, init_x, lr=0.01, step_num=100):\n","  x = init_x\n","\n","  for i in range(step_num):\n","    grad = numerical_gradient(f, x)\n","    x -=lr * grad\n","\n","  return x"],"metadata":{"id":"ii8xd66AcRQ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["f...最適化したい関数\n","\n","init_x...初期値\n","\n","lr...学習率\n","\n","step_num...勾配法による繰り返しの数(この数だけ更新する)"],"metadata":{"id":"t8k_aPXdDO8a"}},{"cell_type":"code","source":["#function_2の最小値を勾配法で求める\n","init_x = np.array([-3.0, 4.0])\n","print(f\"初期値(3,4)から勾配法(lr=0.1): {gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)}\")\n","print(f\"初期値(3,4)から勾配法(lr=10): {gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)}\")\n","print(f\"初期値(3,4)から勾配法(lr=1e-10): {gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)}\")"],"metadata":{"id":"Aa4F6y-QdkP4","executionInfo":{"status":"ok","timestamp":1639637803435,"user_tz":-540,"elapsed":10,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f9742468-dd5a-4a06-e07f-bbc36419fb2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["初期値(3,4)から勾配法(lr=0.1): [-6.11110793e-10  8.14814391e-10]\n","初期値(3,4)から勾配法(lr=10): [ 2.34235971e+12 -3.96091057e+12]\n","初期値(3,4)から勾配法(lr=1e-10): [ 2.34235971e+12 -3.96091057e+12]\n"]}]},{"cell_type":"markdown","source":["学習率が大きすぎると、大きな値へ発散してしまう。\n","\n","学習率が小さすぎると、ほとんど更新されていない。"],"metadata":{"id":"rnD03DhoDO6l"}},{"cell_type":"markdown","source":["### 4.4.2 NNに対する勾配\n","\n","NNでいう勾配とは、重みパラメータに関する損失関数の勾配。"],"metadata":{"id":"E5wRgW07DO43"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"R9PBKvq3DO3O"}},{"cell_type":"markdown","source":["形状が2×3で重み$\\mathbf{W}$をもつNNがあり、損失関数を$L$とすると、重みと勾配は以下のようにかける。\n","\n","$$\\mathbf{W} = \\left( \\begin{matrix}w_{11} & w_{21} & w_{31}\\\\ w_{12} & w_{22} & w_{32} \\end{matrix} \\right) $$\n","$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\left( \\begin{matrix}\\frac{\\partial L}{w_{11}} & \\frac{\\partial L}{w_{21}} & \\frac{\\partial L}{w_{31}}\\\\ \\frac{\\partial L}{w_{12}} & \\frac{\\partial L}{w_{22}} & \\frac{\\partial L}{w_{32}} \\end{matrix} \\right)$$\n","\n","たとえば、$\\frac{\\partial L}{w_{11}}$は$w_{11}$を少し変化させると、損失関数はどれだけ変化するのかを表している。"],"metadata":{"id":"lgIlNbchDO1a"}},{"cell_type":"markdown","source":["簡単なNNに対して勾配を求める例をやってみよう。"],"metadata":{"id":"ePkb3GpwDOzD"}},{"cell_type":"code","source":["##NNに対して勾配を求める例\n","import sys,os\n","sys.path.append(\"/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master\")\n","import numpy as np\n","from common.functions import softmax, cross_entropy_error\n","from common.gradient import numerical_gradient\n","\n","class simpleNet:\n","  def __init__(self):\n","    self.W = np.random.randn(2, 3) #2*3の正規分布に従った乱数\n","\n","  #予測するためのメソッド\n","  def predict(self, x):\n","    return np.dot(x, self.W)\n","\n","  #損失関数の値を求めるメソッド\n","  def loss(self, x, t):\n","    z = self.predict(x)\n","    y = softmax(z)\n","    loss = cross_entropy_error(y, t)\n","    return loss\n","\n","net = simpleNet()\n","print(f\"重みパラメータ: \\n {net.W}\")\n","x = np.array([0.6, 0.9])\n","pred = net.predict(x)\n","print(f\"入力値(0.6, 0.9)の出力値(予測): {pred}\")\n","print(f\"最大値のインデックス: {np.argmax(pred)}\")\n","\n","t = np.array([0, 0, 1]) #正解ラベル\n","print(f\"正解ラベル[0, 0, 1]の誤差関数: {net.loss(x, t)}\")"],"metadata":{"id":"Lo0Mv3QlM4gg","executionInfo":{"status":"ok","timestamp":1639637803435,"user_tz":-540,"elapsed":8,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2b7a0b9-f434-40bd-baf8-25c4456336c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["重みパラメータ: \n"," [[-0.53377288 -1.22275584 -0.25792771]\n"," [ 0.71507717 -0.97840873 -1.52272425]]\n","入力値(0.6, 0.9)の出力値(予測): [ 0.32330573 -1.61422136 -1.52520845]\n","最大値のインデックス: 0\n","正解ラベル[0, 0, 1]の誤差関数: 2.112054405492041\n"]}]},{"cell_type":"code","source":["#勾配を求める\n","def f(W): #Wはダミー\n","  return net.loss(x, t)\n","\n","#上の関数はlambda式で書くといい。\n","#f = lambda w: net.loss\n","\n","dW = numerical_gradient(f, net.W)\n","print(f\"勾配: \\n {dW}\")"],"metadata":{"id":"DQH6noq7Rbjj","executionInfo":{"status":"ok","timestamp":1639637803435,"user_tz":-540,"elapsed":8,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f07fcc71-3e35-4256-80ee-cc6982f97eea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["勾配: \n"," [[ 0.46099527  0.06641087 -0.52740613]\n"," [ 0.6914929   0.0996163  -0.7911092 ]]\n"]}]},{"cell_type":"markdown","source":["## 4.5学習アルゴリズムの実装<112>"],"metadata":{"id":"dOqJuc2HDOw-"}},{"cell_type":"markdown","source":["この４章で、「損失関数」、「ミニバッチ」「勾配」と重要キーワードが出てきたが、こいつらを実際にNNに組み込んでくことを考えてみる。"],"metadata":{"id":"fbcJsfveDOuN"}},{"cell_type":"markdown","source":["NNの学習は以下の4step\n","\n","**step1(ミニバッチ)**\n","\n","訓練データの中からランダムに一部のデータを選びだす。(標本抽出的な)\n","\n","**step2(勾配の計算)**\n","\n","ミニバッチの損失関数を減らすため、各重みパラメータを求める。\n","\n","**step3(パラメータの更新)**\n","\n","重みパラメータを勾配方向に更新していく\n","\n","以上、step1~step3を繰り返す。"],"metadata":{"id":"kOkWmwZPTPCp"}},{"cell_type":"markdown","source":["この方法を、勾配降下法を用いてパラメータ更新を行うが、ミニバッチが無作為に選ばれていることから、**確率的勾配降下法(SGD)**という。"],"metadata":{"id":"ebSlIRGdTPAW"}},{"cell_type":"markdown","source":["### 4.5.1 2層NNのクラス<113>"],"metadata":{"id":"OgQ03Q7eTO9c"}},{"cell_type":"markdown","source":["2層NNを TwoLayerNet という名前のクラスとして実装。"],"metadata":{"id":"_e6IZdSdTO7F"}},{"cell_type":"code","source":["import sys,os\n","sys.path.append(\"/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master\")\n","from common.functions import *\n","from common.gradient import numerical_gradient\n","\n","class TwoLayerNet:\n","  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n","    #重みの初期化\n","    self.params = {}\n","    self.params['W1'] = weight_init_std * \\\n","                       np.random.randn(input_size, hidden_size)\n","    self.params['b1'] = np.zeros(hidden_size)\n","    self.params['W2'] = weight_init_std * \\\n","                       np.random.randn(input_size, hidden_size)\n","    self.params['b2'] = np.zeros(hidden_size)  \n","\n","  #認識(推論)を行う。xは画像データ\n","  def predict(self, x):\n","    W1, W2 = self.params['W1'], self.params['W2']\n","    b1, b2 = self.params['b1'], self.params['b2']\n","\n","    a1 = np.dot(x, W1) + b1\n","    z1 = sigmoid(a1)\n","    a2 = np.dot(z1, W2) + b2\n","    y = softmax(a2)\n","\n","    return y\n","\n","  #x:入力データ, t:教師データ\n","  def loss(self, x, t):\n","    y = self.predict(x)\n","    \n","    return cross_entropy_error(y, t)\n","\n","  #認識精度を求める\n","  def accuracy(self, x, t):\n","    y = self.predict(x)\n","    y = np.argmax(y, axis=1)\n","    t = np.argmax(t, axis=1)\n","\n","    accuracy = np.sum(y == t) / float(x.shape[0])\n","    return accuracy\n","  \n","  #重みパラメータに対する勾配を計算する。\n","  def numerical_gradient(self, x, t):\n","    loss_W = lambda W: self.loss(x, t)\n","\n","    grads = {}\n","    grads[\"W1\"] = numerical_gradient(loss_W, self.params['W1'])\n","    grads[\"b1\"] = numerical_gradient(loss_W, self.params['b1'])\n","    grads[\"W2\"] = numerical_gradient(loss_W, self.params['W2'])\n","    grads[\"b2\"] = numerical_gradient(loss_W, self.params['b2'])\n","\n","    return grads"],"metadata":{"id":"wL7jvf00V5Lz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2つのインスタンス変数params,gradがあり\n","\n","params変数には重みパラメータ\n","\n","grad変数にはparams変数と対応するように各パラメータの勾配が入ってる。"],"metadata":{"id":"csjZJTOUTO4w"}},{"cell_type":"markdown","source":["初期化メソッドでは重みパラメータの初期化を行う。重みパラメータの初期化については後で詳しくやるとして、ここでは、重みはガウス分布に従う乱数という程度の認識で良い。"],"metadata":{"id":"wd6xOBJtTO18"}},{"cell_type":"markdown","source":["numerical_gradientは次章で誤差逆伝播法を用いて高速化する。"],"metadata":{"id":"x9rE8oEWTOzp"}},{"cell_type":"markdown","source":["### 4.5.2ミニバッチ学習の実装<117>"],"metadata":{"id":"smo9BfEqTOyi"}},{"cell_type":"markdown","source":["TwoLayerNetクラスを対象に、MNISTデータセットを使って学習を行います。"],"metadata":{"id":"V3VQmwDyTOwf"}},{"cell_type":"code","source":["import sys,os\n","from dataset.mnist import load_mnist\n","from ch04.two_layer_net import TwoLayerNet\n","\n","(X_train, t_train), (X_test, t_test) = \\\n","  load_mnist(flatten=True, normalize=False)\n","\n","train_loss_list = []\n","\n","#ハイパーパラメータ\n","iters_num = 10000 #繰り返し回数(iteration)\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","for i in range(iters_num):\n","  #ミニバッチの取得\n","  batch_mask = np.random.choice(train_size, batch_size)\n","  x_batch = x_train[batch_mask]\n","  t_batch = t_train[batch_mask]\n","\n","  #勾配の計算\n","  grad = network.numerical_gradient(x_batch, t_batch)\n","  #grad = network.gradient(x_batch, t_batch) # 高速版\n","  \n","  #パラメータの更新\n","  for key in (\"W1\", \"b1\",\"W2\", \"b2\"):\n","    network.params[key] -=learning_rate * grad[key]\n","\n","  #学習経過の記録\n","  loss = network.loss(x_batch, t_batch)\n","  train_loss_list.append(loss)"],"metadata":{"id":"b7T0cpQv_d0I","executionInfo":{"status":"error","timestamp":1639637841336,"user_tz":-540,"elapsed":37908,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"colab":{"base_uri":"https://localhost:8080/","height":420},"outputId":"ea370169-d5a8-45c6-899f-31616688a18c"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-101-953a3aa43ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m#勾配の計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0;31m#grad = network.gradient(x_batch, t_batch) # 高速版\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master/ch04/two_layer_net.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master/common/gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtmp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mfxh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x+h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master/ch04/two_layer_net.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# x:入力データ, t:教師データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master/ch04/two_layer_net.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# x:入力データ, t:教師データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master/ch04/two_layer_net.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["このコードは、ミニバッチサイズを100\n","\n","→毎回60000個の訓練データから、ランダムに100個(batch_mask)のデータを抜き出して\n","\n","→その100個のミニバッチを対象に勾配を求めて、SGDによりパラメータ更新\n","\n","という手順を踏んでいる。"],"metadata":{"id":"n7KAQfbm866H"}},{"cell_type":"markdown","source":["### 4.5.3テストデータで評価\n"],"metadata":{"id":"EKy7lt8F9jmE"}},{"cell_type":"markdown","source":["訓練データの損失関数の値が減ったかどうかだけでは、汎化性能が高いかどうかの判断には繋がらない。\n","overfittingを起こしているかもしれないから。\n","\n","そこで、学習を行う過程で定期的に訓練データとテストデータを対象に認識精度を記録しておこう。\n","\n"],"metadata":{"id":"QioDgpPi95JN"}},{"cell_type":"markdown","source":["定期的に記録を今回は1epochとしておこう。\n","\n","**エポック(epoch)**とは、たとえば1000個の訓練データに対して、10個のミニバッチで学習する場合、SGDを100回繰り返したら、全てのデータを見たことになる。この100回 = 1epoch。"],"metadata":{"id":"niCpD0Py_GCp"}},{"cell_type":"code","source":["#学習を行う過程で定期的に訓練データとテストデータを対象に認識精度を記録\n","import sys,os\n","from dataset.mnist import load_mnist\n","from ch04.two_layer_net import TwoLayerNet\n","\n","(X_train, t_train), (X_test, t_test) = \\\n","  load_mnist(flatten=True, normalize=False)\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","iter_per_epoch = max(train_size / batch_size, 1) #1epochあたりの繰り返し回数\n","\n","#ハイパーパラメータ\n","iters_num = 10000 #繰り返し回数(iteration)\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","for i in range(iters_num):\n","  #ミニバッチの取得\n","  batch_mask = np.random.choice(train_size, batch_size)\n","  x_batch = x_train[batch_mask]\n","  t_batch = t_train[batch_mask]\n","\n","  #勾配の計算\n","  grad = network.numerical_gradient(x_batch, t_batch)\n","  #grad = network.gradient(x_batch, t_batch) # 高速版\n","  \n","  #パラメータの更新\n","  for key in (\"W1\", \"b1\",\"W2\", \"b2\"):\n","    network.params[key] -=learning_rate * grad[key]\n","\n","  #学習経過の記録\n","  loss = network.loss(x_batch, t_batch)\n","  train_loss_list.append(loss)\n","  \n","  #1epochごとの認識精度を計算\n","  if i % iter_per_epoch == 0:\n","    train_acc = network.accuracy(x_train, t_train)\n","    test_acc = network.accuracy(x_test, t_test)\n","    train_acc_list.append(train_acc)\n","    test_acc_list.append(test.acc)\n","    print(\"train acc, test acc |\" + str(train_acc) + \",\" + str(test_acc))"],"metadata":{"id":"aFlLC7eO9i7q"},"execution_count":null,"outputs":[]}]}