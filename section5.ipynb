{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"section5.ipynb","provenance":[],"authorship_tag":"ABX9TyNjjSTj7/n7OE5av8DRO62q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 5章　誤差逆伝播法"],"metadata":{"id":"rpn-nkgNykYJ"}},{"cell_type":"markdown","source":["4章では勾配を数値微分によって求めていたが、この章では、重みパラメータの勾配の計算を効率よく行う手法である**「誤差逆伝播法」**について学んでいく。\n","\n","数式を使って理解していく方法と、計算グラフを使って理解していく2つの方法をやっていこう。"],"metadata":{"id":"msok7MghykV1"}},{"cell_type":"markdown","source":["## 5.1 計算グラフ<123>\n"],"metadata":{"id":"Ev2BATFGykTx"}},{"cell_type":"markdown","source":["### 5.1.1 計算グラフを使って解く<124>"],"metadata":{"id":"0YX5TQ6oVcpF"}},{"cell_type":"markdown","source":["計算グラフを使って問題を解くには以下の手順で進む\n","\n","1:計算グラフを構築する\n","\n","2:計算グラフ上で計算を左から右へ進める。(この左から右へ進めるというのは**順伝播**という。)\n","\n","実際に以下の簡単な2つの問に対して計算グラフを解いてみよう"],"metadata":{"id":"n4jjayn3ykQH"}},{"cell_type":"markdown","source":["問1:太郎君はスーパーで1個100円のリンゴを2個買いました。支払う金額は?消費税は10%とする。"],"metadata":{"id":"FX8k6sjLykOz"}},{"cell_type":"markdown","source":["問2:太郎君はスーパーで1個100円のリンゴを2個,1個150円のミカンを3個買いました。支払う金額は?消費税は10%とする。"],"metadata":{"id":"YFqkPJBLykMs"}},{"cell_type":"markdown","source":["### 5.1.2 局所的な計算<126>"],"metadata":{"id":"LqKDeVBiykKW"}},{"cell_type":"markdown","source":["局所的な計算とは、全体でどのようなことが行われていても、「自分に関係する小さな範囲」だけから次の結果を出力できるということ。\n","\n","実際に例を出してみる。"],"metadata":{"id":"fdOk5SoTykH-"}},{"cell_type":"markdown","source":["図5-4\n","\n","この図においてリンゴとそれ以外の買い物を合計する計算(4000 + 200)は、4000という数字がどのように計算されたかに関して考えずに、ただ2つの数を足している。\n","これが、局所的なということ。"],"metadata":{"id":"6bzg8Ar0W1L1"}},{"cell_type":"markdown","source":["### 5.1.3 なぜ計算グラフで解くのか<127>"],"metadata":{"id":"-RqWVP3hW1Hm"}},{"cell_type":"markdown","source":["計算グラフを使って解くメリットは以下\n","\n","・局所的な計算\n","\n","各ノードでは単純な計算に集中して問題を単純化できる\n","\n","・途中計算をすべて保持できる。\n","\n","・逆方向の伝播によって微分を効率的に行える←**最大のメリット**"],"metadata":{"id":"ecbNT8WAW1Ep"}},{"cell_type":"markdown","source":["上で使った問１を用いて、3つ目のメリット逆伝播の微分を効率的に行えるについて考察していこう。\n","\n","\n"],"metadata":{"id":"w7yrnAHdW1CA"}},{"cell_type":"markdown","source":["問１に対してりんごの値段が値上がりした場合、最終的な支払い金額にどのように影響するのかを調べる。これはすなわち「りんごの値段に対する支払い金額の微分」とみることができる。\n","\n","図５ー５\n","\n","逆伝播は右から左へ「1→1.1→2.2」となっている。（支払い金額の割合とれば求めれる）\n","\n","すなわち、りんごの値段が微小変化したら、その変化の2.2倍だけ増加します。"],"metadata":{"id":"nClPFNLeW0_j"}},{"cell_type":"markdown","source":["## 5.2 連鎖律<129>"],"metadata":{"id":"tJhFXBHnW09D"}},{"cell_type":"markdown","source":["### 5.2.1 計算グラフの逆伝播<129>"],"metadata":{"id":"ycNw3oTPW06_"}},{"cell_type":"markdown","source":["計算グラフを用いた逆伝播の例は以下である。\n","\n","\n","逆伝播の計算手順は、信号$E$に対して、ノードの局所的な微分をかけて、それを次のノードに伝播していく。"],"metadata":{"id":"CAg7M-39W042"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"E4mrwpRgW023"}},{"cell_type":"markdown","source":["### 5.2.2 連鎖律とは<129>"],"metadata":{"id":"EsxnHJGgW00b"}},{"cell_type":"markdown","source":["### 5.2.3 連鎖律と計算グラフ<131>"],"metadata":{"id":"oe5utLTtW0yQ"}},{"cell_type":"markdown","source":["次の式(5.1)に関する連鎖律の計算を計算グラフで表すことを考える。\n","\n","$$z = t^2\\\\t = x + y \\\\ (5.1)$$\n","\n","chain ruleを用いて、$\\frac{\\partial z}{\\partial x}$を求めた結果が以下である。\n","\n","$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = 2t*1 = 2(x + y)$\n","\n","これを図式化したのが以下である。\n","\n","図5-6\n","\n","結局何が言いたいかって、一番左の逆伝播の結果。\n","\n","これは連鎖律より$\\frac{\\partial z}{\\partial z}\\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x} = \\frac{\\partial z}{\\partial x} $がなりたち、「xに関するzの微分」に対応している。"],"metadata":{"id":"huZS_75LW0vf"}},{"cell_type":"markdown","source":["ここまでの計算結果を用いれば計算グラフは以下となる\n","\n","図5-8"],"metadata":{"id":"fJ_Oj2AnW0s1"}},{"cell_type":"markdown","source":["## 5.3 逆伝播<132>\n","\n","「＋」や「×」といった演算に対して逆伝播が成り立つ仕組みに関して考察していこう。"],"metadata":{"id":"Awx3zIpzW0qS"}},{"cell_type":"markdown","source":["### 5.3.1 加算ノードの逆伝播<132>"],"metadata":{"id":"vLE32gpiW0nv"}},{"cell_type":"markdown","source":["$z = x + y$という式に対して逆伝播を見ていこう。\n","\n","まずzに関してx,yそれぞれの偏微分を考えると\n","\n","$$\\frac{\\partial z}{\\partial x} = 1,\\frac{\\partial z}{\\partial y} = 1 \\hspace{10mm}(5.5)$$\n","となる\n","\n","ここで、最終的に$L$という値を出力する大きな計算グラフを考えると、以下のようになる。\n","\n","図5-9\n","\n","このように加算ノードの逆伝播は１を乗算するだけなので、入力値をそのまま次のノードに流すだけ。"],"metadata":{"id":"wNVAXN8ZW0kP"}},{"cell_type":"markdown","source":["### 5.3.2 乗算ノードの逆伝播<134>"],"metadata":{"id":"JEsFRX8OW0gI"}},{"cell_type":"markdown","source":["次に乗算ノードの逆伝播として\n","\n","$z = xy$という式を考える。この式の微分は以下である。\n","\n","$$\\frac{\\partial z}{\\partial x} = y, \\frac{\\partial t}{\\partial y} = x \\hspace{10mm} (5.6)$$"],"metadata":{"id":"F1_sRz57WzvV"}},{"cell_type":"markdown","source":["この乗算の逆伝播を図に表すと以下になる。\n","\n","図5-12\n","\n","具体的に「5×10=50」という計算に対して乗算ノードは以下となる。\n","\n","図5-13\n","\n","つまりひっくり返した形だな。加算の時はそのまま上流の値を流すだけだったので、入力信号の値とか要らなかったけど、乗算の場合はひっくり返すから入力値必須。"],"metadata":{"id":"r9FeNoMvzt9C"}},{"cell_type":"markdown","source":["### 5.3.3 りんごの例<135>"],"metadata":{"id":"NfIrdm-uzt6M"}},{"cell_type":"markdown","source":["改めて最初の２個のリンゴと消費税の問いに対して、この加算ノードと乗算ノードを用いて計算グラフを現そう。\n","\n","図5-14"],"metadata":{"id":"ch89IVyYzt3T"}},{"cell_type":"markdown","source":["## 5.4単純なレイヤの実装<137>"],"metadata":{"id":"oiNatQirztyG"}},{"cell_type":"markdown","source":["計算グラフの乗算ノードを「乗算レイヤ(MulLayer)」、加算ノードを「加算レイヤ(AddLayer)」として実装していこう。"],"metadata":{"id":"XZzSa8wq2ER4"}},{"cell_type":"markdown","source":["### 5.4.1 乗算レイヤの実装<137>"],"metadata":{"id":"2OE7IyUCztve"}},{"cell_type":"code","source":["class MulLayer:\n","  def __init__(self):\n","    self.x = None\n","    self.y = None\n","\n","  #順伝播に対応\n","  def forward(self, x, y):\n","    self.x = x\n","    self.y = y\n","    out = x * y\n","\n","    return out\n","\n","  #逆伝播に対応\n","  def backward(self, dout): #doutは上流から伝わってきた微分\n","    dx = dout * self.y\n","    dy = dout * self.x\n","\n","    return dx, dy"],"metadata":{"id":"mq8JFuia2pWZ","executionInfo":{"status":"ok","timestamp":1639883025705,"user_tz":-540,"elapsed":10,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}}},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":["では、この上さんレイヤを用いて、以下の図の支払額を順伝播、各変数に関する微分を逆伝播で求める。\n","\n","図５−１６"],"metadata":{"id":"yQ2f68EVztsu"}},{"cell_type":"code","execution_count":95,"metadata":{"id":"dOAsqWeEyhEv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639883026425,"user_tz":-540,"elapsed":34,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"outputId":"cc911e87-861e-4bf6-cd45-fcd0496e18dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["支払額: 220.00000000000003\n","りんご微分: 2.2,りんごの個数微分: 110.00000000000001,消費税微分: 200\n"]}],"source":["#初期化\n","apple = 100\n","apple_num = 2\n","tax = 1.1\n","\n","#layer\n","mul_apple_layer = MulLayer()\n","mul_tax_layer = MulLayer()\n","\n","#支払額を順伝播\n","apple_price = mul_apple_layer.forward(apple, apple_num) #順伝播の１個目のノード\n","price = mul_tax_layer.forward(apple_price, tax) #順伝播の2個目のノード\n","\n","print(f\"支払額: {price}\")\n","\n","#各変数に関する微分を逆伝播\n","dprice = 1\n","dapple_price, dtax = mul_tax_layer.backward(dprice)\n","dapple,dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","print(f\"りんご微分: {dapple},りんごの個数微分: {dapple_num},消費税微分: {dtax}\")"]},{"cell_type":"markdown","source":["### 5.4.2 加算レイヤの実装<139>"],"metadata":{"id":"zV6ZMyDS6nzg"}},{"cell_type":"code","source":["class AddLayer:\n","  def __init__(self):\n","    pass\n","\n","  #順伝播に対応\n","  def forward(self, x, y):\n","    out = x + y\n","    return out\n","\n","  #逆伝播に対応\n","  def backward(self, dout): #doutは上流から伝わってきた微分\n","    dx = dout * 1\n","    dy = dout * 1\n","\n","    return dx, dy"],"metadata":{"id":"nmNslYVW5EAl","executionInfo":{"status":"ok","timestamp":1639883026428,"user_tz":-540,"elapsed":34,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}}},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":["加算レイヤは入力値によらないのだったから、initはpassでいいよね"],"metadata":{"id":"nsL5xwRf6_gs"}},{"cell_type":"markdown","source":["加算レイヤは以下のやつで実装していこう\n","\n","図5-17"],"metadata":{"id":"LArToJ1g7bKP"}},{"cell_type":"code","source":["apple = 100\n","apple_num = 2\n","orange = 150\n","orange_num = 3\n","tax = 1.1\n","\n","#layer\n","mul_apple_layer = MulLayer()\n","mul_orange_layer = MulLayer()\n","add_apple_orange_layer = AddLayer()\n","mul_tax_layer = MulLayer()\n","\n","#支払額を順伝播\n","apple_price = mul_apple_layer.forward(apple, apple_num) \n","orange_price = mul_orange_layer.forward(orange, orange_num) \n","all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n","price = mul_tax_layer.forward(all_price, tax)\n","\n","#各変数に関する微分を逆伝播\n","dprice = 1\n","dall_price, dtax = mul_tax_layer.backward(dprice)\n","dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n","dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n","dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","print(f\"支払額: {all_price}\")\n","print(f\"\"\"りんご微分: {dapple},りんごの個数微分: {dapple_num},\n","          ,みかん微分: {dorange},みかんの個数微分: {dorange_num}\n","          ,消費税微分: {dtax}\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m3Pl0DYT7G1P","executionInfo":{"status":"ok","timestamp":1639883026430,"user_tz":-540,"elapsed":35,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"outputId":"82a5d0a7-2650-41c9-82da-79bbe44ccae0"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["支払額: 650\n","りんご微分: 2.2,りんごの個数微分: 110.00000000000001,\n","          ,みかん微分: 3.3000000000000003,みかんの個数微分: 165.0\n","          ,消費税微分: 650\n"]}]},{"cell_type":"markdown","source":["## 5.5活性化レイヤの実装<141>"],"metadata":{"id":"3auslo1woQI_"}},{"cell_type":"markdown","source":["計算グラフの考え方をNNに適応していきたいと思う。"],"metadata":{"id":"wcB9XGyUok8Z"}},{"cell_type":"markdown","source":["### 5.5.1 ReLUレイヤ<141>"],"metadata":{"id":"LhjJJRgioXYO"}},{"cell_type":"markdown","source":["ReLUは次の式で表されるんだった。\n","$$ y =  \\left\\{ \\begin{matrix}x&(x > 0)\\\\ 0&(x \\leq 0) \\end{matrix} \\right. \\hspace{10mm}(5.7)$$\n","\n","式(5.7)から、xに関するyの微分は次のようにかける\n","\n","$$\\frac{\\partial y}{\\partial x} = \\left\\{ \\begin{matrix}1&(x > 0)\\\\ 0&(x \\leq 0) \\end{matrix} \\right. \\hspace{10mm}(5.8)$$\n"],"metadata":{"id":"izJ0t3LWoXWG"}},{"cell_type":"markdown","source":["ReLUレイヤの計算グラフ\n","\n","図５−１８"],"metadata":{"id":"p4ENFeqsoXTi"}},{"cell_type":"code","source":["class ReLU:\n","  def __init__(self):\n","    self.mask = None #maskはtrue/FalseからなるNumpy配列\n","  \n","  def forward(self, x):\n","    self.mask = (x <= 0) \n","    out = x.copy()\n","    out[self.mask] = 0\n","\n","    return out\n","\n","  def backward(self, dout):\n","    dout[self.mask] = 0\n","    dx = dout\n","    \n","    return dx"],"metadata":{"id":"Z5VNPJoQr9j2","executionInfo":{"status":"ok","timestamp":1639883026432,"user_tz":-540,"elapsed":34,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}}},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":["### 5.5.2 Sigmoidレイヤ<143>"],"metadata":{"id":"jje8YAnGoXRL"}},{"cell_type":"markdown","source":["シグモイド関数は以下式(5.9)\n","$$y = \\frac{1}{1 + exp(-x)} \\hspace{10mm}(5.9)$$\n","\n","この式を計算グラフで表すと以下である\n","\n","図5-19"],"metadata":{"id":"94TCNFo5oXPD"}},{"cell_type":"markdown","source":["このシグモイドレイヤでは今までの「乗算レイヤ」「加算レイヤ」に加えて、「exp」「/」が入ってきているので、この二つを先に処理しよう。\n","\n"],"metadata":{"id":"-2fcuOTLoXNz"}},{"cell_type":"markdown","source":["・「/」ノード\n","\n","割り算は$y = \\frac{1}{x}$でかけることから、\n","\n","$\\frac{\\partial y}{\\partial x} = -\\frac{1}{x^2} = -y^{2}$\n","\n","従って逆伝播は、上流の値に対して$-y^{2}$を乗算して下流に流せばOK"],"metadata":{"id":"wkdS5BhVoXLu"}},{"cell_type":"markdown","source":["・「exp」ノード\n","\n"],"metadata":{"id":"huNal2GooXJF"}},{"cell_type":"markdown","source":["expノードでは$y = exp(x)$を表し、その微分は以下でかける。\n","\n","$$\\frac{\\partial y}{\\partial x} =exp(x) \\hspace{10mm}(5.11)$$\n","\n","仕方って、逆伝播は$exp(x)$を乗算して下流に流せばOK"],"metadata":{"id":"oacfSZRuoXGP"}},{"cell_type":"markdown","source":["これらのことを用いるとシグモイドレイヤの計算グラフは以下となる。\n","\n","図5-20\n","\n","簡略化すると以下となる\n","\n","図5-21\n","\n","さらにシグモイドレイヤの逆伝播を微分すると、(yにシグモイド関数を代入したりする。)\n","\n","$\\frac{\\partial L}{\\partial y}y^{2}exp(-x) = \\frac{\\partial L}{\\partial y}y(1-y) \\hspace{10mm}(5.12)$\n","\n","この計算より、シグモイドレイヤの逆伝播は順伝播の出力にしか依存しない。\n","\n","図5-21"],"metadata":{"id":"BqTPBMaZoXEK"}},{"cell_type":"code","source":["class Sigmoid:\n","  def __init__(self):\n","    self.out = None #順伝播での出力を保存する。\n","\n","  def forward(self, x):\n","    out = 1 / (1 + np.exp(-x))\n","    self.out = out\n","\n","    return out\n","  \n","  def backward(self, dout):\n","    dx = dout * (1.0 - self.out) * self.out\n","\n","    return dx"],"metadata":{"id":"LiLIo1fzAAfE","executionInfo":{"status":"ok","timestamp":1639883026435,"user_tz":-540,"elapsed":36,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}}},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":["## Affine/Softmaxレイヤの実装<147>"],"metadata":{"id":"VqpusyADoXCD"}},{"cell_type":"markdown","source":["### 5.6.1Affineレイヤ<147>"],"metadata":{"id":"vDG7b46CoW_z"}},{"cell_type":"markdown","source":["NNの順伝播で行う行列の積は、幾何学では**「アフィン変換」**と呼ばれる。\n","\n","そのアフィン変換を行う処理を「Affineレイヤ」という名前で実装していこう。\n"],"metadata":{"id":"oqdCk4v8oW6P"}},{"cell_type":"markdown","source":["これまでスカラ値で行ってきた計算グラフを「行列」がノード間を伝播する場合についても考えていこう。"],"metadata":{"id":"NSD0F2sKoW3I"}},{"cell_type":"markdown","source":["行列を対象とした逆伝播でも、行列の要素毎に書き出すことで、今までのスカラー値を対象とした計算グラフと同じ手順で求めることができる。\n","\n","図5-25"],"metadata":{"id":"jB-tkdDooW1S"}},{"cell_type":"markdown","source":["### 5.6.2バッチ版Affineレイヤ<150>\n","\n"],"metadata":{"id":"_f_rRNuuoWzx"}},{"cell_type":"markdown","source":["ここまでは、入力が$X$の一つのデータを対象にしていたが、こっからは$N$個のデータをまとめて順伝播することを考える。\n","\n","図５−２７"],"metadata":{"id":"FGZOUAgPoWx6"}},{"cell_type":"code","source":["class Affine():\n","  def __init__(self, W, b):\n","    self.W = W\n","    self.b = b\n","    self.x = None\n","    self.dW = None\n","    self.db = None\n","\n","  def forward(self, x):\n","    self.x = x\n","    out = np.dot(x, self.W) + self\n","\n","    return out\n","\n","  def backward(self, dout): #doutは上流からの微分\n","    dx = np.dot(dout, self.W.T)\n","    self.dW = np.dot(self.x.T, dout)\n","    self.db = np.sum(dout, axis=0)\n","    \n","    return dx\n"],"metadata":{"id":"G4bxKFcmKi0D","executionInfo":{"status":"ok","timestamp":1639883026436,"user_tz":-540,"elapsed":37,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}}},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":["### 5.6.3 Softmax-with-Lossレイヤ<152>"],"metadata":{"id":"rqSyatIOoWwK"}},{"cell_type":"markdown","source":["出力層であるsoftmaxをAffine,ReLUと共に表すと以下図のようになる。\n","\n","ず５−２８\n","\n","NNの正規化しない出力結果は「スコア」と呼ばれます。"],"metadata":{"id":"MruJAaqBoWui"}},{"cell_type":"markdown","source":["Softmaxレイヤを実装していくが、損失関数(交差エントロピー誤差)も含めて「Softmax-with-Loss」という名前で実装していく。\n","\n","計算グラフは以下である。\n","\n","ず５−２９\n","\n","簡易版は以下\n","\n","ず５−３０\n","\n","これは、３クラス分類を考えてる。\n","\n","注目すべきは、softmaxからの逆伝播が「y - t」とめちゃ綺麗。(逆に言えばこうなるように設定されているんだけどね。)"],"metadata":{"id":"AZvCBVx6oWsN"}},{"cell_type":"markdown","source":["具体例を考えよう。\n","\n","教師ラベルが(0, 1, 0) であるデータに対して\n","\n","Softmaxレイヤの出力が(0.3, 0.2. 0.5)であったとする。\n","\n","この場合、softmaxレイヤからの逆伝播は(0.3, -0.8, 0.5)という大きな誤差を伝播する。故にsoftmaxレイヤよりも前のレイヤは大きな誤差から大きな内容を学習する。\n","\n","一方、\n","\n","教師ラベルが(0, 1, 0) であるデータに対して\n","\n","Softmaxレイヤの出力が(0.01, 0.99, 0.0)であったとする。\n","\n","この場合、softmaxレイヤからの逆伝播は(0.01, -0.01, 0)という小さな誤差を伝播する。故にsoftmaxレイヤよりも前のレイヤは小さな誤差から小さな内容を学習する。"],"metadata":{"id":"p3WvDETboWqH"}},{"cell_type":"code","source":["class SoftmaxWithLoss:\n","  def __init__(self):\n","    self.loss = None\n","    self.y = None #softmaxの出力\n","    self.t = None #教師データ\n","\n","  def forward(self, x, t):\n","    self.t = t\n","    self.y = softmax(x)\n","    self.loss = cross_entropy_error(self.y, self.t)\n","\n","    return self.loss\n","\n","  def backward(self, dout=1):\n","    batch_size = self.shape[0]\n","    dx = (self.y - self.t) / batch_size\n","\n","    return dx"],"metadata":{"id":"G1gvrKfRamSH","executionInfo":{"status":"ok","timestamp":1639883026438,"user_tz":-540,"elapsed":38,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}}},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":["## 誤差逆伝播法の実装<156>\n"],"metadata":{"id":"nn4z8zltoWnv"}},{"cell_type":"markdown","source":["### 5.7.1NNの学習の全体図<156>"],"metadata":{"id":"-R0UBrDcoWlc"}},{"cell_type":"markdown","source":["NNの学習手順\n","\n","\n","・step1(ミニバッチ)\n","\n","訓練データの中からランダムに一部のデータを選びだす。(標本抽出的な)\n","\n","・step2(勾配の計算)\n","\n","ミニバッチの損失関数を減らすため、各重みパラメータを求める。\n","\n","・step3(パラメータの更新)\n","\n","重みパラメータを勾配方向に更新していく\n","\n","以上、step1~step3を繰り返す。"],"metadata":{"id":"1WDIBLTFoWf8"}},{"cell_type":"markdown","source":["### 5.7.2誤差逆伝播法に対応したNNの実装<157>"],"metadata":{"id":"jC9kgBAdoWdL"}},{"cell_type":"markdown","source":["4.5で作ったTwoLayerNetに加えていく形で実装していく"],"metadata":{"id":"D-vXT4Wtdoh4"}},{"cell_type":"code","source":["import sys,os\n","sys.path.append(\"/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master\")\n","from common.functions import *\n","from common.gradient import numerical_gradient\n","from collections import OrderedDict #順番つきディクショナリ\n","\n","class TwoLayerNet:\n","  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n","    #重みの初期化\n","    self.params = {}\n","    self.params['W1'] = weight_init_std * \\\n","                       np.random.randn(input_size, hidden_size)\n","    self.params['b1'] = np.zeros(hidden_size)\n","    self.params['W2'] = weight_init_std * \\\n","                       np.random.randn(input_size, hidden_size)\n","    self.params['b2'] = np.zeros(hidden_size)  \n","\n","    #レイヤの生成\n","    self.layers = OrderedDict()\n","    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n","    self.layers['ReLU1'] = ReLU()\n","    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n","\n","    self.lastLayer = SoftmaxWithLoss()\n","\n","  #認識(推論)を行う。xは画像データ\n","  def predict(self, x):\n","    for layer in self.layers.values():\n","      x = layer.forward(x)\n","\n","      return x\n","\n","  #x:入力データ, t:教師データ\n","  def loss(self, x, t):\n","    y = self.predict(x)\n","    return self.lastLayer.forward(y, t)\n","\n","  #認識精度を求める\n","  def accuracy(self, x, t):\n","    y = self.predict(x)\n","    y = np.argmax(y, axis=1)\n","    if t.ndim != 1 : t = np.argmax(t, axis=1)\n","\n","    accuracy = np.sum(y == t) / float(x.shape[0])\n","    return accuracy\n","  \n","  #重みパラメータに対する勾配を計算する。\n","  def numerical_gradient(self, x, t):\n","    loss_W = lambda W: self.loss(x, t)\n","\n","    grads = {}\n","    grads[\"W1\"] = numerical_gradient(loss_W, self.params['W1'])\n","    grads[\"b1\"] = numerical_gradient(loss_W, self.params['b1'])\n","    grads[\"W2\"] = numerical_gradient(loss_W, self.params['W2'])\n","    grads[\"b2\"] = numerical_gradient(loss_W, self.params['b2'])\n","\n","    return grads\n","\n","  def gradient(self, x, y):\n","    #forward\n","    self.loss(x, t)\n","\n","    #backward\n","    dout = 1\n","    dout = self.lastLayer.backward(dout)\n","\n","    layers = list(self.layers.values())\n","    layers.reverse()\n","    for layer in layers:\n","      dout = layer.backward(dout)\n","\n","    grads = {}\n","    grads[\"W1\"] = self.layers['Affine1'].dW\n","    grads[\"b1\"] = self.layers['Affine1'].db\n","    grads[\"W2\"] = self.layers['Affine2'].dW\n","    grads[\"b2\"] = self.layers['Affine2'].db\n","\n","    return grads"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"dpobvzvyn3-V","executionInfo":{"status":"error","timestamp":1639883026443,"user_tz":-540,"elapsed":41,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}},"outputId":"d06ce157-f241-4623-b508-728a7d50c9bd"},"execution_count":102,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-102-04d394b68b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m \u001b[0;31m#順番つきディクショナリ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["レイヤを生成したことは効果絶大！なぜなら、大きなネットワークと作りたければ、このレイヤーを追加するだけでいいのだから。"],"metadata":{"id":"RwQy55NBgY8k"}},{"cell_type":"markdown","source":["### 5.7.3誤差逆伝播法の勾配確認<160>"],"metadata":{"id":"5MHDPtG6h7T1"}},{"cell_type":"markdown","source":["誤差逆伝播法は、効率がいいがミスが発生しがち\n","\n","対して、数値微分は、実装は簡単だが、効率が悪い。\n","\n","本当は誤差逆伝播だけでいきたいところだけど、そんな感じでミスが多いから数値微分を用いて**勾配確認**をする"],"metadata":{"id":"bT5uq9uDh7I7"}},{"cell_type":"code","source":["import sys,os\n","sys.path.append(\"/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master\")\n","from dataset.mnist import load_mnist\n","from ch04.two_layer_net import TwoLayerNet\n","\n","(X_train, t_train), (X_test, t_test) = \\\n","  load_mnist(flatten=True, normalize=False)\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","x_batch = x_train[:3]\n","t_batch = t_train[:3]\n","\n","#勾配の計算\n","grad_numerical = network.numerical_gradient(x_batch, t_batch) #数値計算勾配\n","grad_backprop = network.gradient(x_batch, t_batch) #誤差逆伝播勾配\n","  \n","#各重みの絶対誤差の平均\n","for key in grad_numerical.keys():\n","  diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n","  print(key + f\": {str(diff)}\")"],"metadata":{"id":"HBXox2m5iol5","executionInfo":{"status":"aborted","timestamp":1639883026439,"user_tz":-540,"elapsed":33,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.7.4誤差逆伝播法を使った学習<162>"],"metadata":{"id":"El4w3TnCkdrO"}},{"cell_type":"code","source":["#学習を行う過程で定期的に訓練データとテストデータを対象に認識精度を記録\n","import sys,os\n","sys.path.append(\"/content/drive/MyDrive/DS/deep_learning/deep-learning-from-scratch-master\")\n","from dataset.mnist import load_mnist\n","from ch04.two_layer_net import TwoLayerNet\n","\n","(X_train, t_train), (X_test, t_test) = \\\n","  load_mnist(flatten=True, normalize=False)\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","#ハイパーパラメータ\n","iters_num = 10000 #繰り返し回数(iteration)\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","iter_per_epoch = max(train_size / batch_size, 1) #1epochあたりの繰り返し回数\n","\n","\n","for i in range(iters_num):\n","  #ミニバッチの取得\n","  batch_mask = np.random.choice(train_size, batch_size)\n","  x_batch = x_train[batch_mask]\n","  t_batch = t_train[batch_mask]\n","\n","  #勾配の計算(誤差逆伝播)\n","  grad = network.gradient(x_batch, t_batch) # 高速版\n","  \n","  #パラメータの更新\n","  for key in (\"W1\", \"b1\",\"W2\", \"b2\"):\n","    network.params[key] -=learning_rate * grad[key]\n","\n","  #学習経過の記録\n","  loss = network.loss(x_batch, t_batch)\n","  train_loss_list.append(loss)\n","  \n","  #1epochごとの認識精度を計算\n","  if i % iter_per_epoch == 0:\n","    train_acc = network.accuracy(x_train, t_train)\n","    test_acc = network.accuracy(x_test, t_test)\n","    train_acc_list.append(train_acc)\n","    test_acc_list.append(test.acc)\n","    print(\"train acc, test acc |\" + str(train_acc) + \",\" + str(test_acc))"],"metadata":{"id":"SODkhdvzjsfB","executionInfo":{"status":"aborted","timestamp":1639883026441,"user_tz":-540,"elapsed":35,"user":{"displayName":"テレイージー","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17898297042409907644"}}},"execution_count":null,"outputs":[]}]}